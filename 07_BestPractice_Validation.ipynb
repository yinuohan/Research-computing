{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesFergusson/Introduction-to-Research-Computing/blob/master/7_BestPractice_Optimisation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating you code\n",
    "\n",
    "Once you have written some initial code from your prototype with good documentation and a nice modular structure (and of course carefully tracked by git) you will need to check that it is correct.  There are a few things you can do to make this much easier:\n",
    "\n",
    "- Debug\n",
    "- Error trapping\n",
    "- Unit testing\n",
    "\n",
    "For the first Python has a good inbuilt debugger that will help you, `pdg`\n",
    "\n",
    "## Debugging\n",
    "If you run code that has raised an exception in jupyter or ipython you can use the magic command `%debug` to launch an interactive debugger where you can examine the code line by line.  Here are the most useful commands\n",
    "\n",
    "| Command | Desciption |\n",
    "|---|---|\n",
    "| u | Up |\n",
    "| d | Down|\n",
    "| p | Print |\n",
    "| q | Quit |\n",
    "\n",
    "This is best seen in an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottom_func(x):\n",
    "    y = x**2\n",
    "    return str(x) + \" squared equals \" + str(y)\n",
    "\n",
    "def top_func(z):\n",
    "    result = bottom_func(z)\n",
    "    return result\n",
    "        \n",
    "top_func('7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be turned on automatically with `%pdb on` so whenever an exception is raised the debugger is launched automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the code interactivly line by line using `%run -d` which can be more useful if your code is just wrong rather than breaking.  If you launch code with this then you can step through it with the following commands:\n",
    "\n",
    "| Command | Desciption |\n",
    "|---|---|\n",
    "| n | Next line |\n",
    "| s | Step into function|\n",
    "| c | Continue to run normally |\n",
    "| q | Quit |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -d Examples/UnitTesting/simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the debugger from the command line with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">> python3 -m pdb myscript.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error trapping\n",
    "\n",
    "It is a good idea to write you code so that when errors occours the code reports this to you.  Python is pretty good a providing reaonable error messages but it's good practice to make you own.  One simeple place to check things is whether the input makes sense. So for fibonacci we could add: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(num):\n",
    "    \n",
    "    if not isinstance(num,int):\n",
    "        print(\"non-integer input given to function fibonacci; num=\"+str(num))\n",
    "        return 0\n",
    "    \n",
    "    a=0\n",
    "    b=1\n",
    "    for i in range(num):\n",
    "        a,b = b,a+b\n",
    "    return a\n",
    "\n",
    "fibonacci(3.4)\n",
    "fibonacci(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also check error codes that come back from routines and report on them, for instance reading a file. Better examples are checking inputs are in valid ranges for functions you've created or other things which won't cause python to crash but will mean you code gives the wrong answer, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean(list):\n",
    "    x = 0e0\n",
    "    for item in list:\n",
    "        if item==0:\n",
    "            print(\"Harmonic mean does not exist for data containing zeros.  Returned 'None'\")\n",
    "            return None\n",
    "        \n",
    "        x+= 1e0 / item\n",
    "    x = len(list)/x\n",
    "    \n",
    "    return x\n",
    "\n",
    "list1 = [1,2,3,4,5,6,7,8,9]\n",
    "list2 = [1,2,3,4,5,6,7,8,9,0]\n",
    "print(harmonic_mean(list1))\n",
    "print(harmonic_mean(list2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit testing\n",
    "\n",
    "The best way to debug your code is to catch them before they happen which you can do with unit testing.  The ideas is that you would set up a bunch of tests for the code then everytime you do a commit to master or after doing major edits you run them to check you haven't broken anything.  For the code calculating the integral in the previous notebook you may want to to create tests that check you polynomials are OK (by checking orthonormality) or that the final integral with Gauss-Legendre quadriture is correct for large l (ie set X = $\\delta_{l,200}$ and see if you get the correct answer, 0.000018285996687338485).  \n",
    "\n",
    "Having set up these tests you can then automate them to create a test package that runs, say, before a push to a central repository.  This is standard practise in commercial development enviroments and if you can include them in interview test questions this will put you above the majority of applicants.\n",
    "\n",
    "It's a good idea to get into the habit of adding them for functions, ideally before you write it.  Then you can use them to check your code does what you thought it should.  You will end up spending lots of time checking your code when you are trying to fix bugs so setting up the tests in advance can save alot of time. Luckily in python basic ones are easy to do, you can just add it to the docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "\n",
    "def function(x):\n",
    "    \"\"\"\n",
    "    Calculate x + 2\n",
    "    >>> function(5)\n",
    "    7\n",
    "    \"\"\"\n",
    "    return x+3\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fine for super simple tests but isn't much use once you write functions that process data rather than just a number.  There are alot of packages available but `pytest` is the standard (which is not to say it's the best).  This runs from the terminal and looks for any functions with the name `test_somefunction` or `somefunction_test` then runs them.  These functions should contain some code to run then tests to apply to the outputs using the command `assert` which accepts any booleian argument.  If our function was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtwo(x):\n",
    "    \"\"\"\n",
    "        Add 2 to x\n",
    "    \"\"\"\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our test could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_addtwo():\n",
    "    \"\"\"\n",
    "        Test addtwo\n",
    "    \"\"\"\n",
    "    assert( addtwo(3)==5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are in the files simple.py in the directory `Examples/UnitTesting`. We can test them from the command line using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pytest Examples/UnitTesting/simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests can also be held in seperate files like test_simple.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pytest Examples/UnitTesting/test_simple1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to remember is that floating point arithmitic is not exact so the test of add02 in test_simple2.py fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pytest Examples/UnitTesting/test_simple2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this pytest had a function called `approx` which by default allows a relative tolerance of 1e-6 which is mostly fine and it works on most data objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytest import approx\n",
    "import numpy as np\n",
    "print(0.1 + 0.2 == approx(0.3))\n",
    "print((0.1 + 0.2, 0.2+0.4) == approx((0.3,0.6)))\n",
    "print({'a': 0.1 + 0.2, 'b': 0.2 + 0.4} == approx({'a': 0.3, 'b': 0.6}))\n",
    "print(np.array([0.1, 0.2]) + np.array([0.2, 0.4]) == approx(np.array([0.3, 0.6])))\n",
    "print(np.array([0.1, 0.2]) + np.array([0.2, 0.1]) == approx(0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if you are testing things near zero relative tolerances are useless.  Luckly `approx` also allows you to change the tolerances and make them relative or absolute.  If you specify both, it is true if either are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytest import approx\n",
    "print(1.0001 == approx(1))\n",
    "print(1.0001 == approx(1, rel=1e-3))\n",
    "print(1.0001 == approx(1, abs=1e-3))\n",
    "print(1.0001 == approx(1, rel=1e-5, abs=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify a specific failure message with `fail` ie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytest import fail\n",
    "def test_something():\n",
    "    x = somefunc()\n",
    "    if x in badthings:\n",
    "        fail('A bad thing came back from somefunc()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally note that if we specify no arguments `pytest` looks for files names `test_fimename` or `filename_test` and runs them.  In general is is best practice to put your source code and you tests in different directories as it keeps them seperate and safe.  You should have one test file for each module.  Then running pytest in the test directory will check all your code or you can run on individual modules if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pytest Examples/UnitTesting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Game of life\n",
    "\n",
    "Write a small module that runs Conway's game of life, https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life.  You start with a 2D grid (whose size is to be specified at runtime) where the cells are either 'alive' or 'dead'.  Then the rules for stepping in time are:\n",
    "\n",
    "- Overpopulation. Live cells with more than 3 neighbours die\n",
    "- Underpopulation. Live cells with less than 2 neighbours die\n",
    "- Reproduction. Dead cells with exactly 3 neighbours become live\n",
    "\n",
    "Boundaries are periodic\n",
    "\n",
    "The goal is to:\n",
    "1. Prototype \n",
    "    - It should take a starting set of cells or generate a random board\n",
    "    - Have a \"step\" function which advanced the board one timestep\n",
    "    - Have a \"run\" function which evolves the game n steps and plots/animates the result\n",
    "2. Design unit tests (The \"Oscillators\" from the wikipedia page are good for this)\n",
    "3. THEN! Write the code, try to use debug and include error traps.  You can use your unit tests to validate as you go..\n",
    "\n",
    "You can create all the logic yourself but the following functions will make it much easier: 2D convolution from https://docs.scipy.org/doc/scipy-0.15.1/reference/signal.html, which will count your neighbours, and then https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html for implementing the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
